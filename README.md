# Proyecto Final ‚Äì Regresi√≥n Lineal y Regularizaci√≥n

## Integrantes
- JOEL MATEO MANRIQUE VELASQUEZ
- LUIS ENRRIQUE PEREZ RAMOS

---

## üìå Diferencias observadas entre OLS, Ridge y Lasso

### California Housing
- **OLS (Closed-form)**: Captura la relaci√≥n lineal base y obtiene buen desempe√±o, pero es sensible a la multicolinealidad entre las variables y puede sobreajustar si hay correlaciones fuertes.
- **Ridge**: Introdujo regularizaci√≥n que redujo la varianza del modelo. Observamos que los coeficientes se encogieron de manera m√°s uniforme, logrando un mejor equilibrio entre sesgo y varianza.
- **Lasso**: Mantuvo un Œ± muy peque√±o (‚âà0.001), lo que indica que apenas aplic√≥ penalizaci√≥n. Esto se debe a que ninguna variable fue claramente redundante o innecesaria, por lo que el modelo se comport√≥ casi igual a OLS.

### Bike Sharing (hour.csv)
- **OLS**: Ajust√≥ bien los datos, pero no manej√≥ del todo la complejidad asociada a la estacionalidad y los patrones horarios.
- **Ridge**: Seleccion√≥ un Œ± bastante alto (‚âà49.4), lo que muestra que una regularizaci√≥n fuerte ayud√≥ a controlar la alta correlaci√≥n entre variables temporales (hora, d√≠a, estaci√≥n). Obtuvo un MSE ‚âà19380.
- **Lasso**: Escogi√≥ un Œ± muy bajo (‚âà0.13), mostrando que prefiri√≥ no eliminar variables. Su MSE fue pr√°cticamente igual al de Ridge (‚âà19379), lo que indica que ambos modelos se comportaron de forma muy similar, aunque con distinta fuerza de regularizaci√≥n.

---

## üìå Efecto de la tasa de aprendizaje en Gradient Descent

### California Housing
- Con una tasa de aprendizaje **muy peque√±a**, el descenso de gradiente converge de forma estable, pero muy lentamente.  
- Con una tasa **m√°s alta**, el costo disminuye mucho m√°s r√°pido, aunque si es demasiado grande, el algoritmo puede oscilar o divergir.  
- En este dataset, se observ√≥ que un valor intermedio permiti√≥ alcanzar resultados comparables al OLS cerrado en menos iteraciones.

### Bike Sharing
- Debido a la mayor complejidad y n√∫mero de observaciones, la elecci√≥n del learning rate fue a√∫n m√°s importante.  
- Un valor demasiado alto llev√≥ a oscilaciones en la curva de costo, mientras que valores moderados lograron converger a un error cercano al alcanzado por los m√©todos cerrados y regularizados.

---

## üìå Influencia de k-Fold Cross-Validation en la elecci√≥n de la regularizaci√≥n

### California Housing
- El uso de **k=5 folds** permiti√≥ estimar de manera m√°s robusta el desempe√±o de Ridge y Lasso en distintos subconjuntos de datos.  
- Ridge obtuvo un Œ± ‚âà3.7, lo que indica que una regularizaci√≥n moderada mejoraba el ajuste.  
- Lasso seleccion√≥ un Œ± ‚âà0.001, mostrando que no era necesario eliminar variables en este dataset.

### Bike Sharing
- En este caso, la validaci√≥n cruzada encontr√≥ un **Œ± mucho m√°s alto para Ridge (‚âà49.4)**, evidenciando la necesidad de un control fuerte frente a la multicolinealidad y los patrones estacionales.  
- Lasso, en contraste, mantuvo un Œ± bajo (‚âà0.13), indicando que ninguna variable fue eliminada, aunque la penalizaci√≥n m√≠nima fue suficiente para estabilizar el modelo.  
- En ambos casos, la validaci√≥n cruzada fue clave para ajustar la fuerza de regularizaci√≥n seg√∫n la complejidad y redundancia de las caracter√≠sticas.

---

## üìå Conclusi√≥n General
- En **California Housing**, la regularizaci√≥n apenas mejor√≥ el desempe√±o respecto a OLS, porque el dataset no requer√≠a una fuerte penalizaci√≥n.  
- En **Bike Sharing**, Ridge necesit√≥ un Œ± grande para estabilizar el modelo frente a la alta correlaci√≥n de variables temporales, mientras que Lasso casi no elimin√≥ predictores, pero alcanz√≥ un error similar.  
- La validaci√≥n cruzada fue esencial para adaptar la regularizaci√≥n a cada dataset, mostrando c√≥mo un mismo m√©todo puede comportarse distinto seg√∫n la estructura de los datos.  
- El gradiente descendente confirm√≥ la importancia de elegir adecuadamente la tasa de aprendizaje para garantizar convergencia sin inestabilidad.
